\documentclass[10pt]{beamer}
\usetheme{CambridgeUS}
\usecolortheme{seahorse}
\usepackage{amssymb,amsmath}
\usepackage[utf8]{inputenc}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = blue,
    urlcolor = blue
}

\usepackage[capitalise, noabbrev, nameinlink]{cleveref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{makecell}
% -------- Portada ---------
\title{Our proposal of three different queries to tackle TREC-COVID Challenge}
\author{Grado en Ciencia e Ingeniería de Datos}
\institute{Universidade da Coruña}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}
    \frametitle{Introduction}
    This presentation expounds the work done for addressing \href{https://ir.nist.gov/trec-covid/}{TREC-COVID Challenge}, as well as the implementation details and obtained results:

    \begin{itemize}
        \item Parsing, indexing and querying processes are implemented in \href{https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html}{Java 17}, mainly using \href{https://lucene.apache.org/}{Apache Lucene 8.0.0} library. 
        \item The evaluation metric for all queries is the \href{https://amitness.com/2020/08/information-retrieval-evaluation/\#2-average-precisionap}{average precision at $k$} from the \href{https://ir.nist.gov/trec-covid/data/qrels-covid_d5_j0.5-5.txt}{round-5 relevance judgements} of the TREC-COVID Dataset. Since every query is  evaluated on the \href{https://ir.nist.gov/trec-covid/data/topics-rnd5.xml}{50 different topics} (using the \texttt{query} field), the \href{https://blog.paperspace.com/mean-average-precision/}{mean average precision} is used.
        \item The MAP$@k$ metric is evaluated for $k=10, 100, 1000$ returning $n=100, 1000, 1000$ documents, respectively.
        \item Almost all queries were developed based on \cite{schutze2008introduction} and \cite{croft2010search} theory.
    \end{itemize}
\end{frame}

\begin{frame}{Indexing and querying details}
    \begin{itemize}
        \item In the indexing process, \textit{title}, \textit{authors} (only surnames), \textit{abstract}, \textit{body} and \textit{references title and authors} fields were indexed.
        \item Excluding Query 2, out approaches search in \textit{title}, \textit{abstract} and \textit{body} fields of each document. The \href{https://lucene.apache.org/core/6_0_0/queryparser/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.html}{\texttt{MultiFieldQueryParser}} was used assigning weights to the fields.
        \item Indexing process and some computational-expensive queries are parallelized in order to reduce run-time.
    \end{itemize}
\end{frame}

{
\setbeamerfont{frametitle}{size=\normalsize}
\begin{frame}{Query 1. Language Model with Jelinek Mercer smoothing and query expansion based on probability theory and pseudo-relevance feedback}
    \begin{itemize}
        \item Apache Lucene automatically implements the LM model with Jelinek Mercer smoothing by configuring \href{https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/index/IndexWriter.html}{\texttt{IndexWriter}} and \href{https://lucene.apache.org/core/8_0_0/core/org/apache/lucene/search/IndexSearcher.html}{\texttt{IndexSearcher}} with  \href{https://lucene.apache.org/core/8_6_2/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html}{\texttt{LMJelinekMercerSimilarity}} class.
        \item Once the $n$ documents are returned (our initial results), the re-ranking method is computed by considering that top $n$ documents conform the true relevant document set (pseudo-relevance feedback). Thus, from the Binary Independence Model (BIM) assumptions, we can compute the log-odd ratios of each term $t$:
        
        \[ c_t = \log \frac{|V_t| + 0.5}{|V| - |V_t| +1 } + \underbrace{\log \frac{N}{\text{df}_t}}_{\text{idf}_t} \] 

        where $V$ is the set of relevant documents, $V_t$ is the subset of $V$ the term $t$ appears in and  $\text{df}_t$ and $\text{idf}_t$ are the document frequency and inverse document frequency, respectively, of term $t$.
        
        Then expand the original query $q$ with the top $m$ terms with highest odd ratios (where $m$ can be configured by the user).
    \end{itemize}
\end{frame}

\begin{frame}{Query 2. Vector Space Model using TREC-COVID embeddings and computing KNN corrected with Rocchio \cite{salton1990improving}}
    \begin{itemize}
        \item The TREC-COVID Dataset provides a collection of document embeddings computed by \href{https://arxiv.org/abs/2004.07180}{SPECTER transformer}.
        \item Query embeddings were provided with the assignment.
        \item Add to the base BooleanQuery a \href{https://lucene.apache.org/core/9_0_0/core/org/apache/lucene/search/KnnVectorQuery.html}{KnnVectorQuery} with each d
        \item Once the top $n$ documents are retrieved, we apply pseudo-relevance feedback and consider these top $n$ documents are the true relevant documents.
        \item Then correct the initial query $\vec{q}_0$ with Rocchio algorithm:
        \[ \vec{q}_m = \alpha \vec{q}_0 + \beta \frac{1}{|D_r|} \sum_{\vec{d}\in D_r}\vec{d} - \gamma \frac{1}{|D_{nr}|} \sum_{d\in D_{nr}} \vec{d}, \qquad \alpha, \beta, \gamma \in [0,1]\]
        where $\alpha$, $\beta$ and $\gamma$ are parameters that can be fixed by the user and $D_r$ and $D_{nr}$ are the set of relevant and non-relevant documents, respectively.
        \item Finally compute again cosine similarity between the entire document collection and corrected query embeddings $\vec{q}_m$.
    \end{itemize}
\end{frame}

\begin{frame}{Query 3. LM with Jelinek Mercer smoothing and re-ranking documents using PageRank score}
    \begin{itemize}
        \item \textbf{Hypothesis}: Some articles might reference to other articles that are in the collection, which means that there is a high probability that both have information about the same (or at least similar) topic. 
        \item \textbf{Problem}: We don't know the real relations between articles since in each reference only has a \textit{title} and \textit{authors} field.
        \item \textbf{Idea}: Create a match between title and authors of a reference and a document in the collection (i.e. query with reference title and authors and consider the top $n$ documents, where $n$ should be 1 or 2).
        \item Once the match between references and documents is done, construct the transition matrix and compute the Page Rank score.
        \begin{itemize}
            \item \textbf{Improvement}: Instead of using 1 in position $(i, j)$ if $d_i$ references $d_j$, use the number of times $d_i$ references $d_j$.
        \end{itemize}
        \item Proceed by retrieving the top $n$ documents based on LM and multi-field query parser.
        \item Weight the top $n$ documents score by the Page Rank to obtain a new ranking.
    \end{itemize}
\end{frame}


}

\begin{frame}{Results and conclusions}
    \begin{itemize}
        \item Using a Intel i7-8550U processor with 4 cores, \textcolor{blue}{indexing time} was about 345.078s.
        \item Using an Intel i7-8770U processor with 12 cores, \textcolor{blue}{references searching} and matching with the created index was done in, approximately, 20 minutes.
    \end{itemize}
    
    \begin{table}
        \centering
        \caption{MAP$@k$ results and execution time for our queries}
        \scriptsize
        \begin{tabular}{c|c|c|c}
            \toprule
            & $k=10$ $n=100$ & $k=100$, $n=1000$ & $k=1000$, $n=1000$  \\
            \midrule
            \makecell[c]{probabilistic \\ feedback} & 0.44599995 (27.697s) & 0.06919999 (35.548s) & 0.013114109 (26.008s) \\
            \midrule
            \makecell[c]{KNN \\ with Rocchio} & 0.44399998 (119.451s) &  0.062999986 (120.272s) & 0.012579029 (127.94) \\
            \midrule
            \makecell[c]{Page Rank} & & & \\
        \end{tabular}
    \end{table}

\end{frame}

\begin{frame}{Further work}
    \begin{itemize}
        \item Results might be improved via exploring exhaustive alternatives in order to increase MAP$@k$ metric. 
        \item In this assignment we prioritized the exploring of different IR models for academic purposes.
        \item Explore different weights per field in the \href{https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/search/BooleanQuery.html}{BooleanQuery}:
        \begin{itemize}
            \item When $n$ is big,
        \end{itemize}
    \end{itemize}
    
\end{frame}


\begin{frame}
    \bibliography{bibliography}
    \bibliographystyle{apalike}
\end{frame}

\end{document}